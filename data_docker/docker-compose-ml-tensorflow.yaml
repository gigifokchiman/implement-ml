services:
  tensorflow-serving:
    image: tensorflow/serving:latest
    container_name: tensorflow-serving
    ports:
      - "8501:8501"  # REST API
      - "8500:8500"  # gRPC
    environment:
      - MODEL_NAME=model
      - MODEL_BASE_PATH=/models
    volumes:
      - ./models:/models
    command:
      - --model_config_file=/models/models.config
      - --monitoring_config_file=/models/monitoring.config
      - --rest_api_port=8501
      - --rest_api_timeout_in_ms=5000
      - --enable_batching=true
      - --batching_parameters_file=/models/batching.config
      - --enable_model_warmup=true
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/v1/models/${MODEL_NAME}"]
      interval: 30s
      timeout: 10s
      retries: 3

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
    depends_on:
      - tensorflow-serving

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus

networks:
  default:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data: